= Plaso - test plan

:toc:
:toclevels: 4

:numbered!:
[abstract]
== Summary


[preface]
== Document information
[cols="1,5"]
|===
| Author(s): | Kristinn Gudjonsson <kristinn@log2timeline.net> +
Joachim Metz <joachim.metz@gmail.com> +
Daniel White <onager@deerpie.com>
| Abstract: | Test plan for the plaso project
| Classification: | Public
| Keywords: | Test plan, plaso
|===

[preface]
== License
....
Copyright (C) 2015, log2timeline projects maintainers <log2timeline-maintainers@googlegroups.com>
Permission is granted to copy, distribute and/or modify this document under the
terms of the GNU Free Documentation License, Version 1.3 or any later version
published by the Free Software Foundation; with no Invariant Sections, no
Front-Cover Texts, and no Back-Cover Texts. A copy of the license is included
in the "LICENSE" file.
....

[preface]
== Revision history
[cols="1,1,1,5",options="header"]
|===
| Version | Author | Date | Comments
| 0.0.1 | Joachim Metz | | Initial version based on Google docs version.
|===

:numbered:
== Introduction
In the plaso project several different types of testing is used. This document
is intended to determine where there are still gaps in the testing process and
where testing can be more automated.

== Unit tests
Unit tests provide a way to make sure code changes do not break existing
functionality. The unit test in plaso are defined in the `tests` subdirectory
and `tools/*_test.py` files. All unit tests should pass before a code submit
which is enforced by the review scripts.

The unit tests use either test files (stored in test_data) or mock test data
hard coded into the unit test code files.

=== Code coverage
Currently the plaso project uses coverage in combination with coveralls to
determine the code coverage of the unit tests.

== End-to-end test
The end-to-end tests provide a way to make sure the tools have not regressed
regarding functionality they provide, e.g. does `log2timeline --option-X` still
work. This involves running the tool against various images to see if there are
any issues with particular file parsers or parts of the tool that the unit
tests failed to catch.

This also involves writing down the results of parsing these images, that is
the number of events for each parser that got extracted. This can then be used
to compare different platforms.

When an end-to-end test has been run, it is important to compare result to
those of previous test runs to make sure there is no regression in terms of
number of events parsed.

=== Extract and output

[cols="1,5"]
|===
| *Test case* | extract and output
| *Description* | To extract and output data is the typical usage of plaso. +
The scope of this test is to primarily test the extraction process. +
Output should be tested in more detail in the "output and analyze" tests.
|===

Test script:

1. run log2timeline.py on test data
2. run pinfo on the resulting plaso storage file
3. optionally run pinfo --compare on the resulting plaso storage file and a reference plaso storage file
4. run psort on the resulting plaso storage file

[cols="1,1"]
|===
| *Test configuration* | *Description*
| storage media image - single partition/volume | Extract events from a storage media image
| storage media image - all partitions/volumes | Extract events from a storage media image
| storage media image with BDE | Extract events from a storage media image containing a BitLocker encrypted volume
| storage media image with VSS - single snapshot | Extract events from a storage media image with VSS
| storage media image with VSS - all snapshots | Extract events from a storage media image with VSS
| storage media image with filter (file) | Extract events from a storage media image using a filter file
| directory | Extract events from files in a directory
| file | Extract events from a single non-storage media image file
| mounted path | Extract events from files in mounted paths
| hashing | Extract events and hash data streams.
|===

Example filter file:
....
{sysregistry}/SOFTWARE
{sysregistry}/SAM
{sysregistry}/SYSTEM
/(Users|Documents And Settings)/.+/NTUSER.DAT
/home/.+/.config/google-chrome/.+/History
/Users/.+/Library/Application Support/Google/Chrome/.+/History
/Documents and Settings/.+/Local Settings/Application Data/Google/Chrome/User Data/.+/History
/Users/.+/AppData/Local/Google/Chrome/User Data/.+/History
....

==== Abort scenarios

[cols="1,1"]
|===
| *Scenario* | *Expected outcome*
| user abort | Processing is terminated
| collector process is killed before completion | Processing is terminated - collector died
| collector process is completed but process is killed and hence the queue is flushed | Processing is terminated - due to idle workers
| extraction worker process is killed before completion | Processing is terminated - due to idle workers
| storage writer process is killed before completion | Processing is terminated - storage worker died
| all extraction worker processes are killed before completion | Processing is terminated - due to idle workers
| collector process is running but no path specs are generated | Processing is terminated - due to idle workers
|===

=== output and analyze

[cols="1,5"]
|===
| *Test case* | output and analyze
| *Description* | The scope of this test is to test the various output formats and analysis options.
|===

== Static build tests

[yellow-background]*TODO: migrate remaining documentation*

== Deployment tests

[yellow-background]*TODO: migrate remaining documentation*

== Performance tests

[yellow-background]*TODO: migrate remaining documentation*

